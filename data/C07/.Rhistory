library(devtools)
install_github('mandymejia/fMRIscrub', ref='3.0')
install_github("statsmaths/glmgen", subdir="R_pkg/glmgen")
install_github('mandymejia/fMRIscrub', ref='3.0')
library(clever)
sessionInfo('clever')
#install.packages('gifti')
library(gifti) #readgii
install_github('mandymejia/BayesfMRI', ref='1.8')
install_github('mandymejia/ciftiTools', ref='1.2')
library(devtools)
install_github('mandymejia/ciftiTools', ref='1.2')
install_github('mandymejia/ciftiTools', ref='2.1')
library(ciftiTools)
ciftiTools.setOption('wb_path', '/Applications/workbench/')
install_github('mandymejia/BayesfMRI', ref='1.8')
library(devtools)
install_github('mandymejia/BayesfMRI', ref='1.8')
#library(devtools)
#install_github('mandymejia/fMRIscrub', ref='3.0')
library(fMRIscrub)
#install.packages('gifti')
library(gifti) #readgii
#install_github('mandymejia/ciftiTools', ref='1.2') #2.1
#install.packages("INLA", repos=c(getOption("repos"), INLA="https://inla.r-inla-download.org/R/testing"), dep=FALSE)
library(INLA)
inla.pardiso() #download PARDISO license for fast computation
inla.setOption(pardiso.license = '~/pardiso.lic')
inla.pardiso.check() #verify PARDISO license
#install_github('mandymejia/BayesfMRI', ref='1.8')
library(BayesfMRI)
sessionInfo('BayesfMRI')
main_dir <- '~/Google Drive/My Drive/ALS-BayesianGLM/'
data_dir <- file.path(main_dir,'DataExport202008')
setwd(data_dir)
# subjects <- list.dirs(full.names=FALSE, recursive = FALSE)
# subjects <- subjects[subjects != '']
# save(subjects, file='subjects.Rdata')
load(file='subjects.Rdata') #subjects
groups <- ifelse(grepl('A', subjects), 'ALS', 'HC')
table(groups)
#analyze "real" right hand clenching task
task <- 'RRC'
main_dir <- '~/Google Drive/My Drive/ALS-BayesianGLM/ALS-BayesianGLM-paper/'
#read in design and 1st derivative
X <- as.matrix(read.delim('motor.mat', header=FALSE))[,1:2] #third col is NA
data_dir <- file.path(main_dir,'data')
setwd(data_dir)
load(file='subjects.Rdata') #subjects
groups <- ifelse(grepl('A', subjects), 'ALS', 'HC')
table(groups)
#analyze "real" right hand clenching task
task <- 'RRC'
#read in design and 1st derivative
X <- as.matrix(read.delim('motor.mat', header=FALSE))[,1:2] #third col is NA
design_long <- data.frame(column = rep(c('HRF','dHRF'),each=nrow(X)),
volume = rep(1:nrow(X), 2),
value = c(X[,1],X[,2]))
design_long$column <- factor(design_long$column, levels=c('HRF','dHRF'), labels=c('HRF','HRF Derivative'))
ggplot(design_long, aes(x=volume, y=value, group=column, linetype=column)) +
geom_line() + theme_few() + theme(legend.position = 'bottom', legend.title = element_blank()) +
scale_x_continuous(breaks=seq(0, 140, 40)) + ylab('Arbitrary Units') + xlab('Volume Index')
library(ggplot2)
ggplot(design_long, aes(x=volume, y=value, group=column, linetype=column)) +
geom_line() + theme_few() + theme(legend.position = 'bottom', legend.title = element_blank()) +
scale_x_continuous(breaks=seq(0, 140, 40)) + ylab('Arbitrary Units') + xlab('Volume Index')
library(ggthemes)
ggplot(design_long, aes(x=volume, y=value, group=column, linetype=column)) +
geom_line() + theme_few() + theme(legend.position = 'bottom', legend.title = element_blank()) +
scale_x_continuous(breaks=seq(0, 140, 40)) + ylab('Arbitrary Units') + xlab('Volume Index')
s=subjects[1]
print(s)
isubj <- which(subjects==s)
setwd(file.path(data_dir,s))
#for(s in subjects){
s <- 'C07'
print(s)
isubj <- which(subjects==s)
data_dir <- '~/Google Drive/My Drive/ALS-BayesianGLM/DataExport202008/'
#for(s in subjects){
s <- 'C07'
print(s)
isubj <- which(subjects==s)
setwd(file.path(data_dir,s))
files_s <- list.files(pattern = paste('*',task,'lh','fmri.csv',sep='_'))
visits_s <- sort(substr(files_s, start=5, stop=6)) #get visit numbers
n_visits <- length(visits_s)
n_visits
# thresholds <- c(0,1,2) #activation thresholds (percent signal change)
# U <- length(thresholds)
#
# comptime_df <- comptime_act_df <- active_areas_df <- NULL #for activations computation time
n_visits_all <- n_visits_all2 <- rep(NA, length(subjects)) #before and after QC
rm(isubj)
files_s <- list.files(pattern = paste('*',task,'lh','fmri.csv',sep='_'))
visits_s <- sort(substr(files_s, start=5, stop=6)) #get visit numbers
#if(s=='C09') visits_s <- setdiff(visits_s,'08') #exclude due to bad registration
#limit max number of visits to analyze
if(length(visits_s) > 10){
which.visits <- round(seq(1, length(visits_s), length.out=10)) #roughly equally spaced, including first and last visit
visits_s <- visits_s[which.visits]
}
n_visits <- length(visits_s)
n_visits_all[isubj] <- n_visits
#for(s in subjects){
s <- 'C07'
print(s)
isubj <- which(subjects==s)
setwd(file.path(data_dir,s))
n_visits_all[isubj] <- n_visits
exclude <- NULL #any visits to exclude due to excessive outliers
out_s <- vector('list', length=n_visits)
v=1
print(paste0('visit ',v,' of ',n_visits))
setwd(file.path(data_dir,s))
fname_BOLD_sv_lh <- paste(s,visits_s[v],task,'lh','fmri.csv',sep='_')
fname_BOLD_sv_rh <- paste(s,visits_s[v],task,'rh','fmri.csv',sep='_')
if(!(fname_BOLD_sv_lh %in% list.files())) break
if(!(fname_BOLD_sv_rh %in% list.files())) break
BOLD_sv_lh <- read.csv(fname_BOLD_sv_lh, header=FALSE)[,1:136]
BOLD_sv_rh <- read.csv(fname_BOLD_sv_rh, header=FALSE)[,1:136]
BOLD_sv <- rbind(BOLD_sv_lh, BOLD_sv_rh)
#compute and plot leverage
PClev_sv <- clever(t(BOLD_sv))
library(clever)
#compute and plot leverage
PClev_sv <- clever(t(BOLD_sv))
print(plot(PClev_sv))
#if more than 1/4 outliers, exclude visit (after loop)
out_s[[v]] <- PClev_sv$outlier_flags[[1]]
mean(out_s[[v]])
mean(PClev_sv$outlier_flags$leverage__PCA_kurt)
#if more than 1/4 outliers, exclude visit (after loop)
out_s[[v]] <- PClev_sv$outlier_flags$leverage__PCA_kurt
pct_out <- mean(out_s[[v]])
pct_out
data_dir
data_dir_repo <- '~/Google Drive/My Drive/ALS-BayesianGLM/ALS-BayesianGLM-paper/data'
setwd(data_dir_repo)
#save(visits_s, out_s, exclude, file=paste0('visits_',s,'.RData'))
load(file=paste0('visits_',s,'.RData'))
visits_s
out_s
exclude
n_visits <- length(visits_s)
n_visits_all[isubj] <- n_visits
n_visits
#exclude bad visits
if(!is.null(exclude)){
visits_s <- visits_s[-exclude]
out_s <- out_s[-exclude]
n_visits <- length(visits_s)
}
setwd(file.path(data_dir_repo,s))
results_s <- vector('list', length=2) #save Bayesian GLM results across hemispheres
results0_s <- vector('list', length=2) #save classical GLM results across hemispheres
h=1
hem <- c('lh','rh')[h]
cat(paste0('~~~~~~~~~~~~~~~~~~~ hemisphere ',hem,' ~~~~~~~~~~~~~~~~~~~\n'))
#READ IN PIAL SURFACE FOR SPATIAL MODELING
surf_fname <- paste0(s,'_',hem,'.pial.10K.surf.gii')
surf_sh <- readgii(surf_fname)$data
#READ IN PIAL SURFACE FOR SPATIAL MODELING
surf_fname <- paste0(s,'_',hem,'.pial.10K.surf.gii')
surf_sh <- readgii(surf_fname)$data
vertices_sh <- surf_sh$pointset
faces_sh <- (surf_sh$triangle + 1)
rm(surf_sh)
### READ IN MASK
mask_fname <- paste0(s,'_',hem,'.aparc.ascii.10K.motor.label.csv')
mask_fname
mask_sh <- (as.matrix(read.csv(mask_fname, header=FALSE))==1)
#SET UP SESSIONS LIST
data_sh <- data_sh_mask <- vector('list', n_visits)
names(data_sh) <- names(data_sh_mask) <- visits_s
n_visits
v=1
print(paste0('visit ',v,' of ',n_visits))
#remove outliers from design
keep_sv <- !out_s[[v]]
design_sv <- X[keep_sv,]
#create nuisance regressors & remove outliers
fname_motion_sv <- paste(s,visits_s[[v]],task,'motion.txt',sep='_')
motion_sv <- as.matrix(read.table(fname_motion_sv, header=FALSE))
dmotion_sv <- gradient(motion_sv)[1:136,] #remove extra volumes collected for one subject
motion_sv <- motion_sv[1:136,] #remove extra volumes collected for one subject
trend <- (1:136)/136 #for linear and quadratic time trends
Z <- cbind(motion_sv, dmotion_sv, trend, trend^2)
nuisance_sv <- Z[keep_sv,]
#read BOLD data & remove outliers
fname_BOLD_svh <- paste(s,visits_s[v],task,hem,'fmri.csv',sep='_')
BOLD_svh <- read.csv(fname_BOLD_svh, header=FALSE)[,1:136]
BOLD_svh <- BOLD_svh[,keep_sv] #remove outlier volumes
BOLD_svh_mask <- BOLD_svh[mask_sh[,1],]
### ORGANIZE DATA INTO SESSIONS
data_svh <- list(BOLD = t(BOLD_svh), design=design_sv, nuisance=nuisance_sv)
data_svh_mask <- list(BOLD = t(BOLD_svh_mask), design=design_sv, nuisance=nuisance_sv)
if(!is.session(data_svh)) break
if(!is.session(data_svh_mask)) break
data_sh[[v]] <- data_svh
data_sh_mask[[v]] <- data_svh_mask
rm(data_sh_mask)
#SET UP "SESSIONS" LIST
data_sh <- vector('list', n_visits)
names(data_sh) <- visits_s
for(v in 1:n_visits){
print(paste0('visit ',v,' of ',n_visits))
#remove outliers from design
keep_sv <- !out_s[[v]]
design_sv <- X[keep_sv,]
#create nuisance regressors & remove outliers
fname_motion_sv <- paste(s,visits_s[[v]],task,'motion.txt',sep='_')
motion_sv <- as.matrix(read.table(fname_motion_sv, header=FALSE))
dmotion_sv <- gradient(motion_sv)[1:136,] #remove extra volumes collected for one subject
motion_sv <- motion_sv[1:136,] #remove extra volumes collected for one subject
trend <- (1:136)/136 #for linear and quadratic time trends
Z <- cbind(motion_sv, dmotion_sv, trend, trend^2)
nuisance_sv <- Z[keep_sv,]
#read BOLD data & remove outliers
fname_BOLD_svh <- paste(s,visits_s[v],task,hem,'fmri.csv',sep='_')
BOLD_svh <- read.csv(fname_BOLD_svh, header=FALSE)[,1:136]
BOLD_svh <- BOLD_svh[,keep_sv] #remove outlier volumes
### ORGANIZE DATA INTO SESSIONS
data_svh <- list(BOLD = t(BOLD_svh), design=design_sv, nuisance=nuisance_sv)
if(!is.session(data_svh)) break
data_sh[[v]] <- data_svh
} #end loop over visits
### ESTIMATE CLASSICAL AND BAYESIAN GLM
results0_s[[h]] <- classicalGLM(data=data_sh,
mask=mask_sh,
avg_sessions = FALSE,
scale_design = FALSE)
tmp <- system.time( #6-7 minutes per hemisphere
results_s[[h]] <- BayesGLM(data=data_sh,
beta_names = c('RRC','dRRC'),
vertices=vertices_sh,
faces=faces_sh,
mask=mask_sh,
num.threads = 4,
avg_sessions = FALSE,
scale_design=FALSE))
tmp
288/60
result_dir
mai_dir
main_dir
result_dir <- file.path(main_dir,'results')
save(results_s, results0_s, file=file.path(result_dir, paste0('GLMs_',s,'.RData')))
#compute size of areas of activation
mask_sh_used <- results_s[[h]]$mask
mesh_sh <- results_s[[h]]$mesh
mesh_sh <- inla.mesh.create(loc = as.matrix(vertices_sh), tv = as.matrix(faces_sh))
areas_sh <- diag(inla.fmesher.smorg(mesh_sh$loc,mesh_sh$graph$tv, fem = 0, output = list("c0"))$c0)
areas_sh <- areas_sh[mask_sh_used]
#identify activations based on joint posterior (~2 minutes per visit)
alpha <- 0.05
act0_combined <- matrix(0, length(mask_sh_used), n_visits)
v=1
print(paste0('visit ',v,' of ',n_visits))
#classical GLM activations
act0_FWER <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FWER')
act0_FDR <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FDR')
act0_nocorr <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='none')
act0_combined[,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
act0_combined <- matrix(0, length(mask_sh_used), n_visits)
act0_combined[,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
act0_FWER <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FWER')
act0_FDR <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FDR')
act0_nocorr <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='none')
act0_combined[,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
length(act0_nocorr$active)
length(act0_FDR$active )
length(act0_FWER$active)
dim(act0_combined)
table(mask_sh_used)
load(file.path(main_dir,'Results_2021',fname))
main_dir
load(file.path('~/Google Drive/My Drive/ALS-BayesianGLM/','Results_2021',fname))
fname <- paste0('act0_',s,'_',hem,'.RData')
load(file.path('~/Google Drive/My Drive/ALS-BayesianGLM/','Results_2021',fname))
dim(act0_combined)
act0_combined[mask_sh_used,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
length(act0_combined[mask_sh_used,v])
act0_combined <- matrix(0, length(mask_sh_used), n_visits)
act0_combined[mask_sh_used,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
length(act0_combined[mask_sh_used,v])
length(act0_nocorr$active )
table(mask_sh_used)
table(results0_s[[h]]$mask)
results0_s[[h]]$`01`$mask
table(results0_s[[h]]$`01`$mask)
table(mask_sh)
mask_sh_used0 <- results0_s[[h]][[1]]$mask #for classical GLM
act0_combined <- matrix(0, length(mask_sh_used0), n_visits)
act0_combined[mask_sh_used0,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
fname <- paste0('act0_',s,'_',hem,'.RData')
save(act0_combined, file=file.path(result_dir,fname))
#classical GLM activations
act0_combined <- matrix(0, length(mask_sh_used0), n_visits)
for(v in 1:n_visits){
print(paste0('visit ',v,' of ',n_visits))
act0_FWER <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FWER')
act0_FDR <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FDR')
act0_nocorr <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='none')
act0_combined[mask_sh_used0,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
area_svu_FWER <- sum(areas_sh[act0_FWER$active[mask_sh_used]==1]) #compute size of active area
area_svu_FDR <- sum(areas_sh[act0_FDR$active[mask_sh_used]==1]) #compute size of active area
area_svu_nocorr <- sum(areas_sh[act0_nocorr$active[mask_sh_used]==1]) #compute size of active area
active_areas_df_ii <- data.frame(subject=s,
hemisphere=hem,
visit=visits_s[v],
method=c('FWER','FDR','nocorr'),
threshold=0,
area = c(area_svu_FWER, area_svu_FDR, area_svu_nocorr))
active_areas_df <- rbind(active_areas_df, active_areas_df_ii)
}
# thresholds <- c(0,1,2) #activation thresholds (percent signal change)
# U <- length(thresholds)
#
# comptime_df <- comptime_act_df <- NULL
active_areas_df <- NULL
#classical GLM activations
act0_combined <- matrix(0, length(mask_sh_used0), n_visits)
for(v in 1:n_visits){
print(paste0('visit ',v,' of ',n_visits))
act0_FWER <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FWER')
act0_FDR <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='FDR')
act0_nocorr <- id_activations.classical(model_obj=results0_s[[h]],
field_inds = 1,
session_name = visits_s[v],
threshold=0,
alpha=alpha,
correction='none')
act0_combined[mask_sh_used0,v] <- act0_nocorr$active + act0_FDR$active + act0_FWER$active
area_svu_FWER <- sum(areas_sh[act0_FWER$active[mask_sh_used]==1]) #compute size of active area
area_svu_FDR <- sum(areas_sh[act0_FDR$active[mask_sh_used]==1]) #compute size of active area
area_svu_nocorr <- sum(areas_sh[act0_nocorr$active[mask_sh_used]==1]) #compute size of active area
active_areas_df_ii <- data.frame(subject=s,
hemisphere=hem,
visit=visits_s[v],
method=c('FWER','FDR','nocorr'),
threshold=0,
area = c(area_svu_FWER, area_svu_FDR, area_svu_nocorr))
active_areas_df <- rbind(active_areas_df, active_areas_df_ii)
}
save(act0_combined, file=file.path(result_dir, paste0('act0_',s,'_',hem,'.RData')))
v
v=1
excur_sv <- vector('list', length=U)
thresholds <- c(0,1,2) #activation thresholds (percent signal change)
U <- length(thresholds)
excur_sv <- vector('list', length=U)
v=1
thr_u <- thresholds[u]
u=1
thr_u <- thresholds[u]
thr_u <- thresholds[u]
time_u <- system.time(
excur_sv[[u]] <- id_activations.posterior(model_obj=results_s[[h]],
field_names = 'RRC',
session_name = visits_s[v],
threshold=thr_u,
alpha=alpha))
area_svu <- sum(areas_sh[excur_sv[[u]]$active==1]) #compute size of active area
active_areas_df_ii <- data.frame(subject=s,
hemisphere=hem,
visit=visits_s[v],
method='excur',
threshold=thr_u,
area = area_svu)
active_areas_df <- rbind(active_areas_df, active_areas_df_ii)
load(file=file.path('~/Google Drive/My Drive/ALS-BayesianGLM/','Results_2021','activeareas_df.Rdata'))
head(active_areas_df)
subset(active_areas_df, subject=='C07' & visit=='01', threshold==0, method=='excut')
subset(active_areas_df, subject=='C07' & visit=='01' & threshold==0 & method=='excur')
area_svu
nrow(active_areas_df)
nrow(unique(active_areas_df))
head(unique(active_areas_df))
active_areas_df <- unique(active_areas_df)
nrow(comptime_act_df)
nrow(unique(comptime_act_df))
save(active_areas_df, comptime_act_df, file=file.path(result_dir,'activeareas_df.Rdata'))
time_u
sum(n_visits_all[groups=='ALS'])
